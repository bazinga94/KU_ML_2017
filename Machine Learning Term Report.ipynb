{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model을 만들기 전에 여러가지 기법을 이용하여 test를 해보았습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm   #SVM!!!\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "train_data = pd.read_csv('C:/Users/user/ML Term/train_data.csv')\n",
    "test_data = pd.read_csv('C:/Users/user/ML Term/test_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 처리는 one hot encoding 하고 머신러닝 기법에 따라 normalize 하는게 좋다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_ohe = pd.get_dummies(train_data.iloc[:,:-1])  #protocol 성분을 onehotencoding\n",
    "test_data_ohe = pd.get_dummies(test_data.iloc[:,1:-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = train_data_ohe\n",
    "y = train_data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_X = test_data_ohe\n",
    "test_y = test_data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_cv, X_test_cv, y_train_cv, y_test_cv = train_test_split(X, y, test_size=0.4, random_state=0) #Cross Validation 용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### confusion matrix를 확인한 결과 class r2l, u2r의 데이터가 너무 적고 normal데이터가 너무 많은 관계로 잘 classify가 되지 않는다.  따라서 data를 최대한 균등하게 조절하여 train해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EX = pd.DataFrame(data=train_data, columns=['xAttack'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xAttack = EX['xAttack']\n",
    "xAttack[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dos,normal,probe,r2l,u2r\n",
    "dos_count = 0\n",
    "dos_array = []\n",
    "normal_count = 0\n",
    "normal_array = []\n",
    "probe_count = 0\n",
    "probe_array = []\n",
    "r2l_count = 0\n",
    "r2l_array = []\n",
    "u2r_count = 0\n",
    "u2r_array = []\n",
    "for i in range(0,len(xAttack)):\n",
    "    if(xAttack[i]=='dos'):\n",
    "        dos_array.append(i)\n",
    "        \n",
    "for i in range(0,len(xAttack)):\n",
    "    if(xAttack[i]=='normal'):\n",
    "        normal_array.append(i)\n",
    "        \n",
    "for i in range(0,len(xAttack)):\n",
    "    if(xAttack[i]=='probe'):\n",
    "        probe_array.append(i)\n",
    "        \n",
    "for i in range(0,len(xAttack)):\n",
    "    if(xAttack[i]=='r2l'):\n",
    "        r2l_array.append(i)\n",
    "        \n",
    "for i in range(0,len(xAttack)):\n",
    "    if(xAttack[i]=='u2r'):\n",
    "        u2r_array.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dos_array))\n",
    "print(len(normal_array))\n",
    "print(len(probe_array))\n",
    "print(len(r2l_array))\n",
    "print(len(u2r_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_fs = train_data_ohe.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사용하고 싶은 데이터 수 -> exdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exdata = 800  #사용 하고 싶은 데이터\n",
    "train_data_fs_dos = pd.DataFrame(data=train_data_fs, index=dos_array[0:exdata])\n",
    "train_data_fs_normal = pd.DataFrame(data=train_data_fs, index=normal_array[0:exdata])\n",
    "train_data_fs_probe = pd.DataFrame(data=train_data_fs, index=probe_array[0:exdata])\n",
    "train_data_fs_r2l = pd.DataFrame(data=train_data_fs, index=r2l_array)\n",
    "train_data_fs_u2r = pd.DataFrame(data=train_data_fs, index=u2r_array)\n",
    "\n",
    "test_data_fs_dos = pd.DataFrame(data=y, index=dos_array[0:exdata])\n",
    "test_data_fs_normal = pd.DataFrame(data=y, index=normal_array[0:exdata])\n",
    "test_data_fs_probe = pd.DataFrame(data=y, index=probe_array[0:exdata])\n",
    "test_data_fs_r2l = pd.DataFrame(data=y, index=r2l_array)\n",
    "test_data_fs_u2r = pd.DataFrame(data=y, index=u2r_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_fs_sum = train_data_fs_dos.append(train_data_fs_normal).append(train_data_fs_probe).append(train_data_fs_r2l).append(train_data_fs_u2r)\n",
    "X_ex = train_data_fs_sum\n",
    "\n",
    "test_data_fs_sum = test_data_fs_dos.append(test_data_fs_normal).append(test_data_fs_probe).append(test_data_fs_r2l).append(test_data_fs_u2r)\n",
    "y_ex = test_data_fs_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y_ex) # frame 을 series로 바꿔줘야 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_ex = y_ex.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "decision_tree = tree.DecisionTreeClassifier(criterion='entropy', max_depth=10, random_state=0)\n",
    "decision_tree.fit(X_ex, y_ex) \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_tree = decision_tree.predict(test_X)\n",
    "print('Accuracy: %.3f' % accuracy_score(test_y, y_pred_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 용 application\n",
    "### data를 얼마나 제거 했을때 + feature를 몇개만 사용했을 때 \n",
    "\n",
    "### 성능이 좋을지 test하기 위해 간단한 application 작성\n",
    "\n",
    "### DT에 대해 test data만 봤을때는 데이터 800개 + feature 35개 사용했을때 성능이 좋았다. -> 81.6%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "decision_tree = tree.DecisionTreeClassifier(criterion='entropy', max_depth=10, random_state=0)\n",
    "\n",
    "for exdata in range(1, 200):\n",
    "    exdata = exdata * 50\n",
    "    train_data_fs_dos = pd.DataFrame(data=train_data_fs, index=dos_array[0:exdata])\n",
    "    train_data_fs_normal = pd.DataFrame(data=train_data_fs, index=normal_array[0:exdata])\n",
    "    train_data_fs_probe = pd.DataFrame(data=train_data_fs, index=probe_array)\n",
    "    train_data_fs_r2l = pd.DataFrame(data=train_data_fs, index=r2l_array)\n",
    "    train_data_fs_u2r = pd.DataFrame(data=train_data_fs, index=u2r_array)\n",
    "\n",
    "    test_data_fs_dos = pd.DataFrame(data=y, index=dos_array[0:exdata])\n",
    "    test_data_fs_normal = pd.DataFrame(data=y, index=normal_array[0:exdata])\n",
    "    test_data_fs_probe = pd.DataFrame(data=y, index=probe_array)\n",
    "    test_data_fs_r2l = pd.DataFrame(data=y, index=r2l_array)\n",
    "    test_data_fs_u2r = pd.DataFrame(data=y, index=u2r_array)\n",
    "\n",
    "\n",
    "    train_data_fs_sum = train_data_fs_dos.append(train_data_fs_normal).append(train_data_fs_probe).append(train_data_fs_r2l).append(train_data_fs_u2r)\n",
    "    X_ex = train_data_fs_sum\n",
    "\n",
    "    test_data_fs_sum = test_data_fs_dos.append(test_data_fs_normal).append(test_data_fs_probe).append(test_data_fs_r2l).append(test_data_fs_u2r)\n",
    "    y_ex = test_data_fs_sum\n",
    "\n",
    "    type(y_ex) # frame 을 series로 바꿔줘야 한다\n",
    "\n",
    "    y_ex = y_ex.iloc[:,0]\n",
    "\n",
    "    \n",
    "    decision_tree.fit(X_ex, y_ex) \n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    y_pred_tree = decision_tree.predict(test_X)\n",
    "    if (accuracy_score(test_y, y_pred_tree) > 0.78):\n",
    "        print('사용한 data ', exdata)\n",
    "        print('Accuracy: %.3f' % accuracy_score(test_y, y_pred_tree))\n",
    "\n",
    "        for best in range(5,41):\n",
    "            KBest = SelectKBest(chi2, k=best)\n",
    "            KBest = KBest.fit(X_ex,y_ex)\n",
    "            X_fs = KBest.transform(X_ex)\n",
    "            test_X_fs = KBest.transform(test_X)\n",
    "\n",
    "            model_fs = decision_tree.fit(X_fs, y_ex)\n",
    "\n",
    "            y_pred_erf = model_fs.predict(test_X_fs)\n",
    "            if(accuracy_score(test_y, y_pred_erf) > 0.8):\n",
    "                print('  KBest수 ',best)\n",
    "                print('  FS 하고 Accuracy: %.3f' % accuracy_score(test_y, y_pred_erf))\n",
    "print(\"끝!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_n = normalize(X, norm='l1')   #one hot encoding 을 해줘야 할듯!!!\n",
    "test_X_n = normalize(test_X, norm='l1')   #l1, l2 두 종류가 있다!\n",
    "X_ex_n = normalize(X_ex, norm='l1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RFE \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE\n",
    "\n",
    "Wrapper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recursive Feature Elimination\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn import tree\n",
    "model = tree.DecisionTreeClassifier()\n",
    "\n",
    "#model = LogisticRegression()\n",
    "# create the RFE model and select 3 attributes\n",
    "rfe = RFE(model)       #전체는 41! 35attribute 선택\n",
    "rfe = rfe.fit(X_ex, y_ex)\n",
    "# summarize the selection of the attributes\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_rfe = rfe.predict(test_X)\n",
    "print('Accuracy: %.3f' % accuracy_score(test_y, y_pred_rfe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ranking에 '1'로 표시 되는 feature들을 골라서 training한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive Feature Elimination\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#model = KNeighborsClassifier()\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "rfe = RFE(model)       #전체는 40! dafault일때 자동을 선택??\n",
    "rfe = rfe.fit(X_ex, y_ex)\n",
    "# summarize the selection of the attributes\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_rfe = rfe.predict(test_X)\n",
    "print('Accuracy: %.3f' % accuracy_score(test_y, y_pred_rfe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rfe를 제외한 나머지는 predict를 지원하지 않는다.... 따라서 따로 test_X를 가공 해야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance Threshold\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html#sklearn.feature_selection.VarianceThreshold\n",
    "\n",
    "normalize data 는 사용 불가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#예시\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sample = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]\n",
    "vt = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "vt.fit(sample)\n",
    "vt.fit_transform(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "vt = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "vt = vt.fit(X_ex,y_ex)\n",
    "print(vt.get_support())\n",
    "print(vt.transform(X_ex).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "X_fs = vt.transform(X_ex)\n",
    "test_X_fs = vt.transform(test_X)\n",
    "\n",
    "model_original = ExtraTreesClassifier().fit(X_ex, y_ex)\n",
    "model_fs = ExtraTreesClassifier().fit(X_fs, y_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_erf = model_original.predict(test_X)\n",
    "print('FS 하기 전 Accuracy: %.3f' % accuracy_score(test_y, y_pred_erf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_erf = model_fs.predict(test_X_fs)\n",
    "print('FS 하고 Accuracy: %.3f' % accuracy_score(test_y, y_pred_erf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select KBest\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest\n",
    "\n",
    "Filter method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "KBest = SelectKBest(chi2, k=35)\n",
    "KBest = KBest.fit(X_ex,y_ex)\n",
    "print(KBest.get_support())\n",
    "print(KBest.transform(X_ex).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상위 K 개의 feature를 고른다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "X_fs = KBest.transform(X_ex)\n",
    "test_X_fs = KBest.transform(test_X)\n",
    "\n",
    "model_original = ExtraTreesClassifier().fit(X_ex, y_ex)\n",
    "model_fs = ExtraTreesClassifier().fit(X_fs, y_ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델은 아무거나 사용해도 괜찮다. 성능이 계속 잘 나오는 random forest를 사용해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_erf = model_original.predict(test_X)\n",
    "print('FS 하기 전 Accuracy: %.3f' % accuracy_score(test_y, y_pred_erf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_erf = model_fs.predict(test_X_fs)\n",
    "print('FS 하고 Accuracy: %.3f' % accuracy_score(test_y, y_pred_erf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test용 application\n",
    "KBest를 몇개로 했을때 성능이 좋은지 test해보기 위해 간단한 application 제작\n",
    "\n",
    "기법은 Decision Tree를 사용했다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "decision_tree = tree.DecisionTreeClassifier(criterion='entropy', max_depth=10, random_state=0)\n",
    "\n",
    "for best in range(5,41):\n",
    "    KBest = SelectKBest(chi2, k=best)\n",
    "    KBest = KBest.fit(X_ex,y_ex)\n",
    "    X_fs = KBest.transform(X_ex)\n",
    "    test_X_fs = KBest.transform(test_X)\n",
    "\n",
    "    model_fs = decision_tree.fit(X_fs, y_ex)\n",
    "\n",
    "    y_pred_erf = model_fs.predict(test_X_fs)\n",
    "    if(accuracy_score(test_y, y_pred_erf) > 0.78):\n",
    "        print('KBes수 ',best)\n",
    "        print('FS 하고 Accuracy: %.3f' % accuracy_score(test_y, y_pred_erf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10~15, 27~40 의 feature 를 선택 했을때 성능이 좋았다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select from Model\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel\n",
    "\n",
    "Embedded Method\n",
    "\n",
    "Meta-transformer for selecting features based on importance weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "decision_tree = tree.DecisionTreeClassifier(criterion='entropy', max_depth=10, random_state=0)\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "tree_original = decision_tree.fit(X_ex, y_ex)\n",
    "tree = SelectFromModel(tree_original).fit(X_ex,y_ex)\n",
    "\n",
    "print(tree.get_support())\n",
    "print(tree.transform(X_ex).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fs = tree.transform(X_ex)\n",
    "test_X_fs = tree.transform(test_X)\n",
    "\n",
    "tree_fs = decision_tree.fit(X_ex, y_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_lsvc = tree_original.predict(test_X)\n",
    "print('FS 하기 전 Accuracy: %.3f' % accuracy_score(test_y, y_pred_lsvc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_lsvc = tree_fs.predict(test_X)\n",
    "print('FS 하고 Accuracy: %.3f' % accuracy_score(test_y, y_pred_lsvc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "erf_original = RandomForestClassifier(n_estimators=10, criterion='gini', max_depth=10, max_features=\"auto\").fit(X_ex,y_ex)\n",
    "erf = SelectFromModel(erf_original).fit(X_ex,y_ex)\n",
    "\n",
    "print(erf.get_support())\n",
    "print(erf.transform(X_ex).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_fs = erf.transform(X_ex)\n",
    "test_X_fs = erf.transform(test_X)\n",
    "\n",
    "erf_fs = RandomForestClassifier(n_estimators=10, criterion='gini', max_depth=10, max_features=\"auto\").fit(X_ex, y_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_erf = erf_original.predict(test_X)\n",
    "print('FS 하기 전 Accuracy: %.3f' % accuracy_score(test_y, y_pred_erf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_erf = erf_fs.predict(test_X)\n",
    "print('FS 하고 Accuracy: %.3f' % accuracy_score(test_y, y_pred_erf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection은 KBest로 직접 상위 몇개를 사용 할지 정할 때 성능이 가장 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify 방법들 \n",
    "\n",
    "여기서 성능을 비교하여 Ensemble, feature selection 등등에 사용하자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "decision_tree = tree.DecisionTreeClassifier(criterion='entropy', max_depth=10, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree.fit(X_ex, y_ex) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(decision_tree, X_test_cv, y_test_cv, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_tree = decision_tree.predict(test_X)\n",
    "print('Accuracy: %.3f' % accuracy_score(test_y, y_pred_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labeling 만 했을 때 보다 one hot encoding 하였을 때 성능 조금 향상 \n",
    "\n",
    "L1 은 0.760, L2 normalize 시 0.76 -> 0.727 로 성능이 떨어졌다....\n",
    "\n",
    "decision tree는 normalize를 하지 않는것이 더 좋다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.fit(X_n,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_val_score(gnb, X_test_cv, y_test_cv, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_gnb = gnb.predict(test_X_n)\n",
    "print('Accuracy: %.3f' % accuracy_score(test_y, y_pred_gnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gnb2 = gnb.fit(X_n,y).predict(X)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"% (X.shape[0],(y != y_pred_gnb2).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalize 시  0.289 -> 0.489 로 상승 하지만 그래도 성능이 너무 낮아 사용할 수 없다 L1: 0.480"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr.fit(X_n,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_val_score(lr, X_test_cv, y_test_cv, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_lr = lr.predict(test_X_n)\n",
    "print('Accuracy: %.3f' % accuracy_score(test_y, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one hot encoding 한 뒤 성능 소폭 상승 0.614 -> 0.619, normalize한 뒤 성능이 L2 0.619 -> 0.641로 증가함, L1옵션은 성능이 0.643"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "p = Perceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p.fit(X_n,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_val_score(p, X_test_cv, y_test_cv, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_p = p.predict(test_X_n)\n",
    "print('Accuracy: %.3f' % accuracy_score(test_y, y_pred_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalize L2 한 뒤 성능 대폭 향상 0.119 -> 0.649, L1 옵셥시 0.662로 다시 상승"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(activation='relu', hidden_layer_sizes=(5,5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp.fit(X_ex,y_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_val_score(mlp, X_test_cv, y_test_cv, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_mlp = mlp.predict(test_X)\n",
    "print('Accuracy: %.3f' % accuracy_score(test_y, y_pred_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one hot encoding을 하고 성능 0.699 -> 0.729 으로 향상, normalize 하고 나서는 어째선지 성능이 0.729 ->  0.748로\n",
    "MLP는 학습 할 때마다 성능이 계속 바뀐다.\n",
    "\n",
    "시간이 너무 오래걸려 사용하기 어렵다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_ex,y_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_val_score(knn, X_test_cv, y_test_cv, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_knn = knn.predict(test_X)\n",
    "print('Accuracy: %.3f' % accuracy_score(test_y, y_pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one hot encoding한 뒤 0.736 -> 0.735 normalize 한 뒤 0.735 -> 0.745, L1 시 0.743\n",
    "\n",
    "성능은 괜찮지만 Decision Tree가 더 좋다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "\n",
    "### SVM 코드들은 Data Selection으로 데이터 양을 확 줄인 상태가 아니라면 \n",
    "### Training 시간이 너무 오래 걸려 사용 할 수 없다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm   #SVM은 너무 오래 걸린다.... 자기전에 켜두고 자자\n",
    "clf = svm.SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.fit(X_fs, y_fs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_val_score(clf, X_test_cv, y_test_cv, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_clf = clf.predict(test_X)\n",
    "print('Accuracy: %.3f' % accuracy_score(test_y, y_pred_clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf2 = svm.SVC(decision_function_shape='ovo')\n",
    "clf2.fit(X_fs, y_fs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_clf2 = clf2.predict(test_X)\n",
    "print('Accuracy: %.3f' % accuracy_score(test_y, y_pred_clf2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf3 = svm.SVC(decision_function_shape='ovr')\n",
    "clf3.fit(X_fs,y_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_clf3 = clf3.predict(test_X_n)\n",
    "print('Accuracy: %.3f' % accuracy_score(test_y, y_pred_clf3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lin_clf = svm.LinearSVC()\n",
    "lin_clf.fit(X_fs, y_fs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_lin_clf = lin_clf.predict(test_X)\n",
    "print('Accuracy: %.3f' % accuracy_score(test_y, y_pred_lin_clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clfr = svm.SVR()\n",
    "clfr.fit(X_fs, y_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_clfr = clfr.predict(test_X_n)\n",
    "print('Accuracy: %.3f' % accuracy_score(test_y, y_pred_clfr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_svc = svm.SVC(kernel='linear')\n",
    "linear_svc.kernel\n",
    "linear_svc.fit(X_fs,y_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_linear_svc = linear_svc.predict(test_X_n)\n",
    "print('Accuracy: %.3f' % accuracy_score(test_y, y_pred_linear_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rbf_svc = svm.SVC(kernel='rbf')\n",
    "rbf_svc.kernel\n",
    "rbf_svc.fit(X_n,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_rbf_svc = rbf_svc.predict(test_X_n)\n",
    "print('Accuracy: %.3f' % accuracy_score(test_y, y_pred_rbf_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사용할 만한 기법은 decision tree, MLP, KNN 이다. \n",
    "\n",
    "### SVM은 성능이 70프로대가 나오긴하나 시간이 너무 오래 걸려 사용할 수 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging(KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "bagging_knn = BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=0.5)\n",
    "\n",
    "#bagging with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_knn.fit(X_ex, y_ex)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_val_score(bagging_knn, X_test_cv, y_test_cv, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_bagging_knn = bagging_knn.predict(test_X)\n",
    "print('Accuracy: %.3f' % accuracy_score(test_y, y_pred_bagging_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one hot encoding 후 0.743 -> 0.749, normalize 후 0.749 -> 0.737 로 성능이 떨어진게 아니라 다시 0.747!! MLP와 마찬가지로 측정할때 마다 값이 왔다갔다 함...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging(DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "decision_tree = DecisionTreeClassifier(criterion='entropy', max_depth=10, random_state=0)\n",
    "bagging_knn = BaggingClassifier(decision_tree, max_samples=1.0, max_features=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_knn.fit(X_ex, y_ex)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_bagging_knn = bagging_knn.predict(test_X)\n",
    "print('Accuracy: %.3f' % accuracy_score(test_y, y_pred_bagging_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging은 Decision Tree로 시도 했을때 성능이 가장 좋았다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ab = AdaBoostClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "ab = AdaBoostClassifier(DecisionTreeClassifier(criterion='entropy', max_depth=10, random_state=0), n_estimators=100, learning_rate=1)\n",
    "ab.fit(X_ex, y_ex)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_val_score(ab, X_test_cv, y_test_cv, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_ab = ab.predict(test_X)\n",
    "print('Accuracy: %.3f' % accuracy_score(test_y, y_pred_ab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost도 Decision tree를 이용할 때 성능이 좋았다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Voting 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hard voting\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "h1 = KNeighborsClassifier()\n",
    "h2 = RandomForestClassifier(random_state=1)\n",
    "h3 = MLPClassifier()\n",
    "\n",
    "hvote = VotingClassifier(estimators=[('knn', h1), ('rf', h2), ('mlp', h3)], voting='hard')\n",
    "\n",
    "for clf, label in zip([h1, h2, h3, hvote], ['KNN', 'Random Forest', 'MLP', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hvote.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_hvote = hvote.predict(test_X)\n",
    "print('Accuracy: %.3f' % accuracy_score(test_y, y_pred_hvote))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Voting 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Soft Voting\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from itertools import product\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Training classifiers\n",
    "s1 = KNeighborsClassifier()\n",
    "s2 = RandomForestClassifier(random_state=1)\n",
    "s3 = MLPClassifier()\n",
    "\n",
    "svote = VotingClassifier(estimators=[('dt', s1), ('knn', s2), ('svc', s3)], voting='soft', weights=[2,1,2]) #여기서 weight를 준다. \n",
    "\n",
    "s1.fit(X,y)\n",
    "s2.fit(X,y)\n",
    "s3.fit(X,y)\n",
    "svote.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### voting은 Hard Voting만 사용했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, criterion='entropy', max_depth=10, max_features=\"auto\") \n",
    "# n_estimator가 tree의 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_ex, y_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_val_score(rf, X_test_cv, y_test_cv, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_rf = rf.predict(test_X)\n",
    "print('Accuracy: %.3f' % accuracy_score(test_y, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "erf = ExtraTreesClassifier(n_estimators=1000, criterion='entropy', max_depth=10, max_features=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erf.fit(X_ex,y_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_val_score(erf, X_test_cv, y_test_cv, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_rf = erf.predict(test_X)\n",
    "print('Accuracy: %.2f' % accuracy_score(test_y, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 성능이 보통 Random Forest 보다 Extra tree classifier가 더 잘 나온다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest 를 활용한 그래프 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_evaluation import plot\n",
    "%matplotlib inline\n",
    "plot.feature_importances(rf, top_n=5)\n",
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.confusion_matrix(test_y, rf.predict(test_X)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classify한 형태를 보면 class 2 가 다른 데이터에 비해 압도적으로 많아서?? 대부분 classify가 class 2로 쏠리는 것을 볼 수 있다. \n",
    "그리고 class r2l, u2r 은 normal과 data가 거의 유사한지 잘 detect를 하지 못한다. \n",
    "\n",
    "-> class 분포를 조금 균등하게 해서 classify 해봐야겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.precision_recall(test_y, rf.predict_proba(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.roc(test_y, rf.predict_proba(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve, validation_curve\n",
    "train_sizes, train_scores, test_scores = learning_curve(rf, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.learning_curve(train_scores, test_scores, train_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_range = np.array([1, 10, 100])\n",
    "param_name = \"n_estimators\"\n",
    "train_scores, test_scores = validation_curve(rf, test_X, test_y, param_name=param_name, param_range=param_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.validation_curve(train_scores, test_scores, param_range, param_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 최종 모델!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm   #SVM!!!\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "train_data = pd.read_csv('C:/Users/user/ML Term/train_data.csv')  #training에 사용할 데이터\n",
    "test_data = pd.read_csv('C:/Users/user/ML Term/test_data.csv')   #test에 사용할 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_ohe = pd.get_dummies(train_data.iloc[:,:-1])  #protocol 성분을 onehotencoding\n",
    "test_data_ohe = pd.get_dummies(test_data.iloc[:,1:-1]) \n",
    "\n",
    "X = train_data_ohe\n",
    "y = train_data.iloc[:,-1]\n",
    "\n",
    "test_X = test_data_ohe\n",
    "test_y = test_data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "45927\n",
    "67343\n",
    "11656\n",
    "995\n",
    "52\n",
    "\n",
    "class 별 원래 data 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EX = pd.DataFrame(data=train_data, columns=['xAttack'])\n",
    "xAttack = EX['xAttack']\n",
    "#dos,normal,probe,r2l,u2r\n",
    "dos_count = 0\n",
    "dos_array = []\n",
    "normal_count = 0\n",
    "normal_array = []\n",
    "probe_count = 0\n",
    "probe_array = []\n",
    "r2l_count = 0\n",
    "r2l_array = []\n",
    "u2r_count = 0\n",
    "u2r_array = []\n",
    "for i in range(0,len(xAttack)):\n",
    "    if(xAttack[i]=='dos'):\n",
    "        dos_array.append(i)\n",
    "        \n",
    "for i in range(0,len(xAttack)):\n",
    "    if(xAttack[i]=='normal'):\n",
    "        normal_array.append(i)\n",
    "        \n",
    "for i in range(0,len(xAttack)):\n",
    "    if(xAttack[i]=='probe'):\n",
    "        probe_array.append(i)\n",
    "        \n",
    "for i in range(0,len(xAttack)):\n",
    "    if(xAttack[i]=='r2l'):\n",
    "        r2l_array.append(i)\n",
    "        \n",
    "for i in range(0,len(xAttack)):\n",
    "    if(xAttack[i]=='u2r'):\n",
    "        u2r_array.append(i)\n",
    "        \n",
    "train_data_fs = train_data_ohe.copy()\n",
    "\n",
    "exdata = 800  #사용 하고 싶은 데이터양\n",
    "\n",
    "train_data_fs_dos = pd.DataFrame(data=train_data_fs, index=dos_array[0:exdata])\n",
    "train_data_fs_normal = pd.DataFrame(data=train_data_fs, index=normal_array[0:exdata])\n",
    "train_data_fs_probe = pd.DataFrame(data=train_data_fs, index=probe_array[0:exdata])\n",
    "train_data_fs_r2l = pd.DataFrame(data=train_data_fs, index=r2l_array[0:exdata])\n",
    "train_data_fs_u2r = pd.DataFrame(data=train_data_fs, index=u2r_array)\n",
    "\n",
    "test_data_fs_dos = pd.DataFrame(data=y, index=dos_array[0:exdata])\n",
    "test_data_fs_normal = pd.DataFrame(data=y, index=normal_array[0:exdata])\n",
    "test_data_fs_probe = pd.DataFrame(data=y, index=probe_array[0:exdata])\n",
    "test_data_fs_r2l = pd.DataFrame(data=y, index=r2l_array[0:exdata])\n",
    "test_data_fs_u2r = pd.DataFrame(data=y, index=u2r_array)\n",
    "\n",
    "train_data_fs_sum = train_data_fs_dos.append(train_data_fs_normal).append(train_data_fs_probe).append(train_data_fs_r2l).append(train_data_fs_u2r)\n",
    "X_ex = train_data_fs_sum\n",
    "\n",
    "test_data_fs_sum = test_data_fs_dos.append(test_data_fs_normal).append(test_data_fs_probe).append(test_data_fs_r2l).append(test_data_fs_u2r)\n",
    "y_ex = test_data_fs_sum\n",
    "\n",
    "type(y_ex) # frame 을 series로 바꿔줘야 한다\n",
    "y_ex = y_ex.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True False  True False  True False  True  True\n",
      "  True False  True  True  True False False False  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True False  True  True]\n",
      "(3252, 35)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "KBest = SelectKBest(chi2, k=35)\n",
    "KBest = KBest.fit(X_ex,y_ex)\n",
    "print(KBest.get_support())\n",
    "print(KBest.transform(X_ex).shape)\n",
    "\n",
    "X_fs = KBest.transform(X_ex)\n",
    "test_X_fs = KBest.transform(test_X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging(DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Accuracy: 0.793\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "decision_tree = DecisionTreeClassifier(criterion='entropy', max_depth=10, random_state=0)\n",
    "bagging = BaggingClassifier(decision_tree, max_samples=1.0, max_features=1.0)\n",
    "\n",
    "bagging.fit(X_fs, y_ex)    \n",
    "y_pred_bagging = bagging.predict(test_X_fs)\n",
    "print('Bagging Accuracy: %.3f' % accuracy_score(test_y, y_pred_bagging))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost(DT) 최종 모델에는 사용 X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy: 0.777\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "ab = AdaBoostClassifier(n_estimators=100)\n",
    "ab = AdaBoostClassifier(DecisionTreeClassifier(criterion='entropy', max_depth=10, random_state=0), n_estimators=100, learning_rate=1)\n",
    "\n",
    "ab.fit(X_fs, y_ex)  \n",
    "y_pred_ab = ab.predict(test_X_fs)\n",
    "print('AdaBoost Accuracy: %.3f' % accuracy_score(test_y, y_pred_ab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision_tree Accuracy: 0.817\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "decision_tree = tree.DecisionTreeClassifier(criterion='entropy', max_depth=10, random_state=0)\n",
    "\n",
    "decision_tree.fit(X_fs, y_ex) \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_tree = decision_tree.predict(test_X_fs)\n",
    "print('Decision_tree Accuracy: %.3f' % accuracy_score(test_y, y_pred_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Accuracy: 0.789\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "#rf = RandomForestClassifier(n_estimators=10, criterion='entropy', max_depth=10, max_features=\"auto\") \n",
    "erf = ExtraTreesClassifier(n_estimators=500, criterion='entropy', max_depth=10, max_features=\"auto\") \n",
    "# n_estimator가 tree의 크기\n",
    "\n",
    "erf.fit(X_fs, y_ex)\n",
    "y_pred_erf = erf.predict(test_X_fs)\n",
    "print('Extra Accuracy: %.3f' % accuracy_score(test_y, y_pred_erf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "\n",
    "hvote = VotingClassifier(estimators=[('bagging', bagging), ('decision_tree', decision_tree), ('extra random forest', erf)], voting='hard')\n",
    "\n",
    "# 각각의 Cross validation \n",
    "# for clf, label in zip([bagging, ab, erf, hvote], ['bagging', 'adaboost', 'extra random forest', 'Ensemble']):\n",
    "#     scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n",
    "#     print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting 했을 때 Accuracy: 0.812\n"
     ]
    }
   ],
   "source": [
    "hvote.fit(X_fs,y_ex)\n",
    "y_pred_hvote = hvote.predict(test_X_fs)\n",
    "print('Voting 했을 때 Accuracy: %.3f' % accuracy_score(test_y, y_pred_hvote))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가장 성능이 잘 나왔던 Decision Tree를 Bagging 한 것과 기본 Decision Tree,  Extra Tree Classifier 중에 Voting을 하여 최종 classify를 하는 모델을 만들었습니다. \n",
    "\n",
    "### Feature Selection 과 Data Selection은 Decision Tree의 결과가 가장 좋았던 수치를 기준으로 수행 하였습니다. \n",
    "### 과제에서 주어진 Test Data로 성능을 측정 했을때 \n",
    "### Decision Tree 단일 모델이 81.7%이었고, 최종 모델로 Voting 했을때 79% ~ 82% 의 성능을 보여주었습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
